\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {3}Learning coupled dictionaries}{41}{chapter.3}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{chap_dictionarylearning}{{3}{41}{Learning coupled dictionaries}{chapter.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.1}From bases to dictionaries}{41}{section.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.1}Predefined dictionaries}{42}{subsection.3.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1.2}Adaptive dictionaries}{42}{subsection.3.1.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.2}Proper Orthogonal Decomposition (POD) as a representation of turbulent fields}{43}{section.3.2}}
\@writefile{toc}{\contentsline {section}{\numberline {3.3}Dictionary learning as a new representation}{45}{section.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.1}Redundant dictionary and sparse representation}{45}{subsection.3.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3.2}Dictionary learning methods}{46}{subsection.3.3.2}}
\@writefile{toc}{\contentsline {subsubsection}{Alternate optimization}{47}{section*.49}}
\@writefile{toc}{\contentsline {subsubsection}{K-SVD}{47}{section*.50}}
\@writefile{toc}{\contentsline {subsubsection}{Online dictionary learning (ODL)}{48}{section*.51}}
\@writefile{toc}{\contentsline {section}{\numberline {3.4}Learning coupled dictionaries}{48}{section.3.4}}
\newlabel{joint_learning}{{3.4}{48}{Learning coupled dictionaries}{section.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.1}Patch-based approach}{48}{subsection.3.4.1}}
\newlabel{subsec:patch_based_estimation}{{3.4.1}{48}{Patch-based approach}{subsection.3.4.1}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Online dictionary learning algorithm by \citet {mairal2010online}\relax }}{49}{algorithm.caption.52}}
\newlabel{algo_ODL}{{3}{49}{Online dictionary learning algorithm by \citet {mairal2010online}\relax }{algorithm.caption.52}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.1}{\ignorespaces  An example of single image super-resolution using coupled dictionary learning and patch-wise approach. Photo credit: \citet {yang2010image}.\relax }}{50}{figure.caption.53}}
\newlabel{fig:ScSR}{{3.1}{50}{An example of single image super-resolution using coupled dictionary learning and patch-wise approach. Photo credit: \citet {yang2010image}.\relax }{figure.caption.53}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.2}The approach}{50}{subsection.3.4.2}}
\newlabel{sec:chap3_theapproach}{{3.4.2}{50}{The approach}{subsection.3.4.2}{}}
\newlabel{eq:DL_approach1}{{3.22}{50}{The approach}{equation.3.4.22}{}}
\newlabel{eq:DL_approach2}{{3.23}{51}{The approach}{equation.3.4.23}{}}
\newlabel{eq:DL_approach3}{{3.24}{51}{The approach}{equation.3.4.24}{}}
\newlabel{eq:DL_approach4}{{3.25}{51}{The approach}{equation.3.4.25}{}}
\newlabel{eq:DL_approach5}{{3.26}{51}{The approach}{equation.3.4.26}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.3}Joint learning methods}{52}{subsection.3.4.3}}
\newlabel{sec:joint_learning_methods}{{3.4.3}{52}{Joint learning methods}{subsection.3.4.3}{}}
\newlabel{eq:jointlearning1}{{3.27}{52}{Joint learning methods}{equation.3.4.27}{}}
\newlabel{eq:jointlearning2}{{3.29}{52}{Joint learning methods}{equation.3.4.29}{}}
\newlabel{eq:jointlearning3}{{3.30}{52}{Joint learning methods}{equation.3.4.30}{}}
\newlabel{eq:jointlearning4}{{3.31}{53}{Joint learning methods}{equation.3.4.31}{}}
\newlabel{eq:jointlearning5}{{3.32}{53}{Joint learning methods}{equation.3.4.32}{}}
\newlabel{eq:jointlearning6}{{3.33}{53}{Joint learning methods}{equation.3.4.33}{}}
\newlabel{eq:jointlearning7}{{3.34}{53}{Joint learning methods}{equation.3.4.34}{}}
\newlabel{eq:jointlearning8}{{3.35}{53}{Joint learning methods}{equation.3.4.35}{}}
\@writefile{lot}{\contentsline {table}{\numberline {3.1}{\ignorespaces  Notations for three different methods of coupled dictionaries learning as proposed by \citet {yang2010image,zeyde2012single}. The dimension of LR and HR patches are $ q$ and $ p$ respectively.\relax }}{54}{table.caption.54}}
\newlabel{tab:DLapproaches}{{3.1}{54}{Notations for three different methods of coupled dictionaries learning as proposed by \citet {yang2010image,zeyde2012single}. The dimension of LR and HR patches are $ \dimpsl $ and $ \dimpsh $ respectively.\relax }{table.caption.54}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4.4}Reconstruction using learned dictionaries}{54}{subsection.3.4.4}}
\newlabel{eq:DLrec1}{{3.36}{54}{Reconstruction using learned dictionaries}{equation.3.4.36}{}}
\newlabel{eq:DLrec2}{{3.37}{54}{Reconstruction using learned dictionaries}{equation.3.4.37}{}}
\newlabel{eq:DLrec3}{{3.38}{54}{Reconstruction using learned dictionaries}{equation.3.4.38}{}}
\newlabel{eq:DLrec4}{{3.39}{54}{Reconstruction using learned dictionaries}{equation.3.4.39}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {4}{\ignorespaces Coupled dictionary learning for reconstruction of high-resolution fields from low-resolution ones\relax }}{55}{algorithm.caption.55}}
\newlabel{algo_SR}{{4}{55}{Coupled dictionary learning for reconstruction of high-resolution fields from low-resolution ones\relax }{algorithm.caption.55}{}}
\newlabel{eq:algo_SR_1}{{3.40}{55}{Coupled dictionary learning for reconstruction of high-resolution fields from low-resolution ones\relax }{equation.3.4.40}{}}
\newlabel{eq:algo_SR_2}{{3.41}{55}{Coupled dictionary learning for reconstruction of high-resolution fields from low-resolution ones\relax }{equation.3.4.41}{}}
\newlabel{eq:algo_SR_3}{{3.42}{55}{Coupled dictionary learning for reconstruction of high-resolution fields from low-resolution ones\relax }{equation.3.4.42}{}}
\newlabel{eq:algo_SR_4}{{3.43}{55}{Coupled dictionary learning for reconstruction of high-resolution fields from low-resolution ones\relax }{equation.3.4.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.5}Dictionary learning for isotropic turbulence fields}{55}{section.3.5}}
\@writefile{lot}{\contentsline {table}{\numberline {3.2}{\ignorespaces  Set of parameters for coupled dictionaries approach (notations are consistent with Algorithm \ref  {algo_SR}): sparsity constrains $ \lambda _1 $ for learning and $ \lambda _2 $ for reconstruction; dimension of LR patches $ q$ and HR patches $ p$; number of atoms $ K$; number of training patches $ m$ \relax }}{56}{table.caption.56}}
\newlabel{tab:DLparams}{{3.2}{56}{Set of parameters for coupled dictionaries approach (notations are consistent with Algorithm \ref {algo_SR}): sparsity constrains $ \lambda _1 $ for learning and $ \lambda _2 $ for reconstruction; dimension of LR patches $ \dimpsl $ and HR patches $ \dimpsh $; number of atoms $ \dimdict $; number of training patches $ \dimptl $ \relax }{table.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.1}On the choice of parameters}{56}{subsection.3.5.1}}
\newlabel{subsec:DL_choice_params}{{3.5.1}{56}{On the choice of parameters}{subsection.3.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.2}Efficiency of representations: a comparative study}{57}{subsection.3.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.2}{\ignorespaces  Dictionaries leaned by PCA and ODL from the set of HR patches of size $ p= 16 \times 16 $. With ODL, only 256 atoms are chosen from 512 atoms.\relax }}{58}{figure.caption.57}}
\newlabel{fig:PODvsDL}{{3.2}{58}{Dictionaries leaned by PCA and ODL from the set of HR patches of size $ \dimpsh = 16 \times 16 $. With ODL, only 256 atoms are chosen from 512 atoms.\relax }{figure.caption.57}{}}
\newlabel{eq:DL_efficiency3}{{3.44}{58}{Efficiency of representations: a comparative study}{equation.3.5.44}{}}
\newlabel{eq:DL_efficiency4}{{3.45}{58}{Efficiency of representations: a comparative study}{equation.3.5.45}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.3}{\ignorespaces  Sparsity vs error outside (left) and zoom in the region of low sparsity at semi-log scale (right) for different representations: wavelet (Daubechies), PCA, dictionary learning (ODL or KSVD). Wavelet transform is for the whole fields of size $ 96 \times 96$, while PCA and DL are for patches of size $ p= 16 \times 16 $.\relax }}{59}{figure.caption.58}}
\newlabel{fig:Sparsity_vs_NRMSE}{{3.3}{59}{Sparsity vs error outside (left) and zoom in the region of low sparsity at semi-log scale (right) for different representations: wavelet (Daubechies), PCA, dictionary learning (ODL or KSVD). Wavelet transform is for the whole fields of size $ 96 \times 96$, while PCA and DL are for patches of size $ \dimpsh = 16 \times 16 $.\relax }{figure.caption.58}{}}
\newlabel{eq:DL_efficiency2}{{3.46}{59}{Efficiency of representations: a comparative study}{equation.3.5.46}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.4}{\ignorespaces  Coupled dictionary of high and low resolution patches, with LR patches are of size $ 4\times 4 $, directly subsampled from their equivalent HR patches of size $ 16 \times 16 $. The regularization parameter $ \lambda $ for join learning are chosen such that about 16 non-zero coefficients are retained for reconstructing the joint patches of the training.\relax }}{60}{figure.caption.60}}
\newlabel{fig:D_HR_LR}{{3.4}{60}{Coupled dictionary of high and low resolution patches, with LR patches are of size $ 4\times 4 $, directly subsampled from their equivalent HR patches of size $ 16 \times 16 $. The regularization parameter $ \lambda $ for join learning are chosen such that about 16 non-zero coefficients are retained for reconstructing the joint patches of the training.\relax }{figure.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.3}Reconstruction of high resolution fields- subsampling cases}{60}{subsection.3.5.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.5}{\ignorespaces  Coupled dictionary of high and low resolution patches, where LR patches are of size $ 4\times 4 $, directly subsampled from their equivalent HR patches of size $ 16 \times 16 $. The regularization parameter $ \lambda $ for join learning are chosen such that about 16 non-zero coefficients are retained for reconstructing the joint patches of the training.\relax }}{61}{figure.caption.61}}
\newlabel{fig:D_HR_HRinterp}{{3.5}{61}{Coupled dictionary of high and low resolution patches, where LR patches are of size $ 4\times 4 $, directly subsampled from their equivalent HR patches of size $ 16 \times 16 $. The regularization parameter $ \lambda $ for join learning are chosen such that about 16 non-zero coefficients are retained for reconstructing the joint patches of the training.\relax }{figure.caption.61}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.6}{\ignorespaces Dictionary of the residual between HR and interpolated LR.\relax }}{62}{figure.caption.63}}
\newlabel{fig:SR_dictionary_residual}{{3.6}{62}{Dictionary of the residual between HR and interpolated LR.\relax }{figure.caption.63}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5.4}Reconstruction of high resolution fields- the downsampling case}{62}{subsection.3.5.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.7}{\ignorespaces Means and standard deviations of NRMSEs estimated between reference and reconstructed fields of all middle planes (at the center of blocks bounded by the two LTHS planes). The reconstructions are by spline interpolation and super-resolution using three different methods:: \textit  {SR1}, \textit  {SR2} and \textit  {SR3} (see table \ref  {tab:DLapproaches}). NRMSEs are $ 0.267 \pm 0.021 $, $ 0.235 \pm 0.019 $, $ 0.233 \pm 0.018 $ and $ 0.230 \pm 0.019 $ respectively. The NRSME of spline interpolation in the equivalent subsampling case is 0.276 (dashed black line).\relax }}{63}{figure.caption.64}}
\newlabel{fig:NRMSE_compare_all_spacespacing_04}{{3.7}{63}{Means and standard deviations of NRMSEs estimated between reference and reconstructed fields of all middle planes (at the center of blocks bounded by the two LTHS planes). The reconstructions are by spline interpolation and super-resolution using three different methods:: \textit {SR1}, \textit {SR2} and \textit {SR3} (see table \ref {tab:DLapproaches}). NRMSEs are $ 0.267 \pm 0.021 $, $ 0.235 \pm 0.019 $, $ 0.233 \pm 0.018 $ and $ 0.230 \pm 0.019 $ respectively. The NRSME of spline interpolation in the equivalent subsampling case is 0.276 (dashed black line).\relax }{figure.caption.64}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.8}{\ignorespaces 2D spectra of all planes used to computed the NRMSEs in figure \ref  {fig:NRMSE_compare_all_spacespacing_04}, from reference, interpolation and SR by three different methods: \textit  {SR1}, \textit  {SR2} and \textit  {SR3} (see table \ref  {tab:DLapproaches}). For scales from $ 0.5k_c $ to $ 1.5 k_c$, energy losses of SR fields compared to reference ones is $ 24\% $, $ 22\% $ and $ 21\% $ respectively, while that of interpolation is $85 \% $.\relax }}{64}{figure.caption.65}}
\newlabel{fig:spectra2d_spacespacing_04}{{3.8}{64}{2D spectra of all planes used to computed the NRMSEs in figure \ref {fig:NRMSE_compare_all_spacespacing_04}, from reference, interpolation and SR by three different methods: \textit {SR1}, \textit {SR2} and \textit {SR3} (see table \ref {tab:DLapproaches}). For scales from $ 0.5k_c $ to $ 1.5 k_c$, energy losses of SR fields compared to reference ones is $ 24\% $, $ 22\% $ and $ 21\% $ respectively, while that of interpolation is $85 \% $.\relax }{figure.caption.65}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3.9}{\ignorespaces (Left) 2D spectra of errors, which are the different between the reference and reconstruction by interpolation and three different SR methods as described in \ref  {sec:joint_learning_methods}. (Right) 2D spectra of errors normalized by the energy spectrum of the reference (the black curve in figure \ref  {fig:spectra2d_spacespacing_04}).\relax }}{65}{figure.caption.66}}
\newlabel{fig:errspectra2d_nonnormalized_normalized_timespacing_06_spacespacing_04}{{3.9}{65}{(Left) 2D spectra of errors, which are the different between the reference and reconstruction by interpolation and three different SR methods as described in \ref {sec:joint_learning_methods}. (Right) 2D spectra of errors normalized by the energy spectrum of the reference (the black curve in figure \ref {fig:spectra2d_spacespacing_04}).\relax }{figure.caption.66}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3.6}Concluding remarks}{65}{section.3.6}}
\@setckpt{corps/dictionarylearning}{
\setcounter{page}{68}
\setcounter{equation}{47}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{0}
\setcounter{mpfootnote}{0}
\setcounter{YAD@abstracts}{2}
\setcounter{YAD@warnings}{0}
\setcounter{part}{0}
\setcounter{chapter}{3}
\setcounter{section}{6}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{9}
\setcounter{table}{2}
\setcounter{tcbbreakpart}{1}
\setcounter{tcblayer}{0}
\setcounter{max@tocdepth}{2}
\setcounter{max@secnumdepth}{2}
\setcounter{su@anzahl}{0}
\setcounter{parentequation}{0}
\setcounter{DTLrowi}{2}
\setcounter{DTLrowii}{2}
\setcounter{DTLrowiii}{0}
\setcounter{DTLrow}{20}
\setcounter{lips@count}{0}
\setcounter{float@type}{32}
\setcounter{FBl@b}{0}
\setcounter{FRobj}{0}
\setcounter{FRsobj}{0}
\setcounter{FBcnt}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{lstnumber}{1}
\setcounter{vrcnt}{0}
\setcounter{Item}{0}
\setcounter{bookmark@seq@number}{0}
\setcounter{LT@tables}{0}
\setcounter{LT@chunks}{0}
\setcounter{subfigure}{0}
\setcounter{lofdepth}{1}
\setcounter{subtable}{0}
\setcounter{lotdepth}{1}
\setcounter{ALG@line}{4}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{algorithm}{4}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{197}
\setcounter{maxnames}{2}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextrayear}{2}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{textcitecount}{2}
\setcounter{textcitetotal}{2}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{FBLTpage}{0}
\setcounter{lstlisting}{0}
\setcounter{section@level}{1}
}
