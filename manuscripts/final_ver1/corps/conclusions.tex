\chapter*{Conclusions and perspectives} 
\label{chap_conclusion_perspectives} 
This work lies in between the two research domains of turbulence and image processing. The main objective was to explore a large spectrum of approaches to estimate small-scale turbulence from given measurements at large scales only. One contribution of the thesis is to review conventional methods. We have also adapted other models inspired from recent works in image processing and proposed new methods to our context. 

The estimation of small-scale turbulence from measurements has been addressed via two problems described in chapter \ref{chap_problem_definition}. The first problem is to find a relationship between large and small scales of turbulence given training samples at all scales. The second is to measure and combine complementary information, which are from space and time in this case. DNS data of an isotropic turbulence and a channel flow are used to setup different numerical experiments. These data give access to all scales of turbulence, which are used as the reference to qualify different approaches. Low-resolution fields of large scales are virtually extracted from the reference ones to which the reconstructions are compared. 

For the two problems, we have proposed two families of approaches. The first one is to learn an empirical relationship between large and small scales through simple regression models or through learning coupled representations of different scales using dictionary learning. The second group of methods is based on the fusion of information. Different assumptions are exploited to propose schemes to combine available measured data. Two fusion models are developed, which use either similarity of structures in the flows or probabilistic models to find compromise estimates given the measurements. 

Chapter \ref{chap_linearregression} reviewed regression models, which find mapping functions between low-resolution and high-resolution fields. The function can be linear (via a set of coefficients) or nonlinear (using fixed kernel spaces). Model performances are usually sensitive to some hyper-parameters. This problem was addressed by optimizing a bias-variance trade-off using the cross-validation technique. Comparing reconstruction results, these models work better than standard spline interpolation. Nonlinear regressions also give more accurate reconstruction than linear ones. 

Chapter \ref{chap_dictionarylearning} discussed \textit{dictionary learning}, a generalization of principal component analysis (called \textit{proper orthogonal decomposition} in turbulence studies) to learn redundant bases that better represent turbulent fields in a sparse manner. A couple of representations are learned and used to reconstruct the high-resolution fields from low-resolution measurements. In the case of direct subsampling that could happen in real experiments, the aliasing problem appears. We observe that the coupled dictionary learning method does not permit to de-alias. The model has not brought significant improvements compared to spatial interpolation. Once avoiding this problem by a prefiltering step, dictionary learning gives significant improvements compared to interpolation. In the range of scale between 0.5 and 1.5 the cutoff, it reduces the energy loss by about $ 60 \% $ and the total error by about $ 15 \% $. 

Chapter \ref{chap_NLM} discussed a non-local means-based propagation model as the first fusion model. The model was based on the hypothesis of \textit{rapid distortion}, which assumes that small scales are advected by large ones. These small-scale information from the LTHS planes are propagated in time based on the similarity level of large scales in space. The model works very well at low subsampling ratios in space and time (corresponding to about $ 1\% $ of energy loss) where it significantly improves the reconstruction accuracy (by about $ 40\% $) compared to single interpolation in space. However, the model is not very robust when the energy losses are more severe. With large subsampling ratios in space (for instance $ \dimsh/\dimsl =6 \times 6 $), it suffers from severe losses and can not recover completely even at large-scale. With large ratios in time (for instance $ \dimth/\dimtl = 8 $) , similarities of large scales decay rapidly and make propagation models less accurate.

To further exploit all available information from measurements, a simple yet efficient fusion model was proposed in chapter \ref{chap_BayesianFusion}. The model estimates a high-resolution field that maximizes its \textit{posterior} probability given the measurements. A Bayesian framework is used and further simplifications lead to a linear fusion formula, a weighted sum of the two single interpolations from the two measured data. Weighted coefficients are covariance matrices of unknown small scales, which are learned from measurements. The model reconstructs high-resolution fields with significant improvement compared to single interpolations from either sources. The superiority of this model is emphasized in the case where energy losses are balanced in space and time. In such cases, improvements are observed both at large and small scales.

Chapter \ref{chap_comparisons} provided a more global view of all proposed methods. We synthesized reconstructions by all approaches for the same setups where measurements in space and time are subsampled in a balanced manner. Dictionary learning is omitted in this comparison because of its slightly different configuration of numerical experiments. Single models to solve the reconstruction problem can be predefined (interpolation) or learned adaptively from the training data (regression). Adaptive models reconstruct the fields more accurately. More significant improvements are observed when combining complementary measurements in space and time. NLM-based model gives very accurate reconstructions at low energy losses. At higher subsampling ratios, this model is not able to over-perform temporal interpolation. Bayesian fusion model is simpler yet more efficient, and gives the best reconstruction in all cases. 

\subsection{Suggestions for future works}
The main purpose of this work was a first exploration of a set of methods for the reconstruction of fully resolved turbulent fields from low resolution measurements. Turbulent fields are highly disordered and much less regular than natural images, therefore the inverse problem of reconstruction is even more difficult in our case. This thesis has opened many new directions to develop tools for turbulence studies. Results have suggested also some new directions for signal/image processing studies to provide even more adapted tools. The present work gives rise to the following suggestions for future works.

\subsubsection*{Extending to the reconstruction of three-component velocity fields?} 
The whole thesis has dealt with the reconstruction of streamwise velocity fields. All results can be reproduced for the other two components independently. However, all components are connected physically through Navier-Stokes equations. Further investigations are needed to exploit the information such as cross-correlation between components, vorticities and divergence of the velocity fields. One idea is to impose the prior of \textit{divergence-free}. Another idea would be to use vorticities as features instead of derivatives to learn coupled dictionaries.  

\subsubsection*{Ensemble of models as generalizing Bayesian fusion model?} 
Chapter \ref{chap_comparisons} has compared performances of all proposed models. It has shown that different models, though exploiting different sources of information or assumptions, give better results than single interpolations. Combining multi-sources of measurements through their interpolations gives more accurate reconstructions compared to one complex single model. This observation suggests to further ensemble different models and take advantage of each single model. Regressions, by learning an adaptive interpolator, could replace spline interpolations. NLM-based propagation models by exploiting the similarity information also appear as a good candidate to replace simple interpolation. 

\subsubsection*{Highly nonlinear mapping function between large and small scales of turbulence?} This thesis has reviewed regression models, but they remain rather simplistic to describe this highly nonlinear relation between scales. This is suggested when observing advantages of kernel regressions in chapter \ref{chap_comparisons}. Other ideas, especially \textit{neural network} and \textit{deep learning} \citep{dong2014image}, could be studied. Such approaches permit much more complex and highly nonlinear mapping functions between input low-resolution and output high-resolution fields, not restricted to a fixed kernel space as the KRR model. However, such a model requires an extremely large amount of training samples. The patch-wise approach is potentially a good candidate to give access to more samples and to localize the information. Such models could be combined with sparse prior \citep{wang2015deep} discussed in chapter \ref{chap_dictionarylearning} and proven to further improve reconstruction accuracy. 

\subsubsection*{Dictionary learning for other inverse problems in turbulence?} As a more efficient representation compared to PCA and predefined wavelets, dictionary learning has demonstrated its possibility to solve the inverse problem of reconstructing high-resolution velocities from low-resolution measurements. The improvement is significant when aliasing is handled carefully but inoperative in the case of direct subsampling. This approach is not limited to super-resolution only. Other inverse problems such as removing noise or estimating missing pixels could be studied. One particular idea is to apply dictionary learning to D time-resolved experimental data of ``\textit{Shake-The-Box}''-PIV \citep{schroder2015advances}. By following the particles, the method resolves till pixel size. However, velocity fields in a uniform grid is estimated by simple interpolations. This could be done with dictionary learning. Viewing the fields with many missing pixels as random sensing, the approach could learn the missing small scales from the position it ``\textit{sees}'' and propose better estimates than interpolation. Also, measurements noise could be separated from the true information in the same step. 

\subsubsection*{Dictionary learning to deal with aliasing from direct subsampling?} Aliasing is a known problem when the sensing system directly subsamples the fields without the prefiltering step. The first attempt of dictionary learning to reconstruct the high-resolution fields from the subsampled measurements has failed potentially due to aliasing. Without its presence, the model gives clear improvements compared to single interpolation. The problem of removing aliasing could be addressed and solved using dictionary learning as well. Coupled dictionaries could be learned to represent the fields with and without aliasing. The model in chapter \ref{chap_dictionarylearning} would become a three-stage super-resolution model, with an intermediate step to remove aliasing.

\subsubsection*{A more complete fusion model, a Bayesian way?} The fusion model proposed in this work uses only a very simple weighted sum formula. The prior of the estimated field is omitted from the formula due to the assumption of a non-informative prior. Full covariance matrices to represent all sources of space-time correlations are also simplified to diagonal ones. Some work remains to be done to further exploit the information from measurements by building more complete covariance matrices. Also, a good prior could further improve further the reconstruction. The prior could carry physical properties of the flow, for example divergence free, energy spectra or even full Navier-Stokes equations. In case the full fields are given as training data, covariance matrices could also be learned adaptively. In such cases, the weights of the fusion model could be learned to take the properties of turbulence into account. 

\subsubsection*{Combining fusion and dictionary learning?} Dictionary learning has shown to be a good representation of turbulent fields using a sparsity prior. This prior helps in solving the ill-posed problem of reconstructing high-resolution fields. Fusion models by combining multi-sources of measurements are simple but also very efficient to solve the problem. The idea of combining sparse prior and fusion could be studied, inspired by \citet{wei2015hyperspectral}. In chapter \ref{chap_BayesianFusion}, the optimization problem is:
\begin{equation}
	\z = \argmin_{\z} \left\lbrace \frac{1}{2} \Mdist{\z -\Interp_t\x}{\Sigma_{\h_t}} + \frac{1}{2}\Mdist{\z -\Interp_s\y}{\Sigma_{\h_s}} \right\rbrace
\end{equation}
By imposing the sparse prior, the problem could be rewriten as:
\begin{equation}
	\z = \dict \dictco \subjectto \{\dict,\dictco\} = \argmin_{\dict,\dictco} \left\lbrace \frac{1}{2} \Mdist{\dict \dictco -\Interp_t\x}{\Sigma_{\h_t}} + \frac{1}{2}\Mdist{\dict\dictco -\Interp_s\y}{\Sigma_{\h_s}} + \lambda \normone{\dictco} \right\rbrace
\end{equation}
or avoiding the interpolations, hence not bringing aliasing terms into the system, as:
\begin{equation}
		\z = \dict \dictco \subjectto \{\dict,\dictco\} = \argmin_{\dict,\dictco} \left\lbrace \frac{1}{2} \Mdist{\Sub_t\dict \dictco -\x}{\Sigma_{\h_t}} + \frac{1}{2}\Mdist{\Sub_s\dict\dictco -\y}{\Sigma_{\h_s}} + \lambda \normone{\dictco} \right\rbrace
\end{equation}
Solving the above highly non-convex problem could be addressed thanks to recent progresses in solving alternating optimization problems \citep{boyd2011distributed,parikh2014proximal}. 

\subsubsection*{Increase resolution of PIV measurements using different setups?} Regression and interpolation are restricted to reconstruct high-resolution fields from low-resolution ones at the same scene, while it is not the case for coupled dictionaries learning. One very promising setup would be to combine high-resolution PIV with time-resolved PIV (Tr-PIV). High-resolution PIV can measure the flow at a small field-of-view but at very high spatial resolution. These measurements could be used to train the dictionaries. Then Tr-PIV measures the flow at a much larger field-of-view but at lower resolution. Using the trained dictionaries, one could estimate time-resolved velocity fields at large field-of-view and high spatial resolution. One should notice also the limitation of such an approach in de-aliasing to carefully design the sensing system. Another configuration could be to use two different PIV systems to measure HTLS and LTHS fields (see figure \ref{fig:space-time_measurements}) and apply fusion models to maximize the level of useful information.

\subsubsection*{Co-conception design of experiments?} The thesis has studied the performances of various models for different configurations at various subsampling ratios. This gives an idea of which method to choose for one particular setup. From a certain loss of energy due to subsamplings, this work suggests the maximum level of accuracy and scale one could expect to reconstruct. This information is very useful to design new challenging experiments in order to maximize the expected level of small scale content after post-processing. This thesis connects to the current research area of co-conception. The idea is to co-design the measurement system with respect to some pre-defined post-processing procedure. 